{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9675d4-20f9-4c58-9a4f-4c128f86f161",
   "metadata": {},
   "source": [
    "# NSD preprocessing code\n",
    "Notebook is mostly based on [this excellent code by mmhenderson](https://github.com/mmhenderson/modfit/blob/master/code/utils/nsd_utils.py).\n",
    "\n",
    "This code can do two things:\n",
    "1. It can take the raw NSD data, z-score it voxel wise within each session, and average it across repeats of the same image. The output here is an array of 1D vectors, where the 1D vector is the flattened brain 3D voxel grid.\n",
    "2. It can take the images shown to the subjects, and then resize then to 256 and store the images into a HDF5 format.\n",
    "\n",
    "You do need to clone the [github here](https://github.com/mmhenderson/modfit) and adjust the path below.\n",
    "You also need to adjust the paths listed in the codebase. Please sign the data agreement to get the NSD data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e7670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/afluo/DiffusionInception/modfit/code\")\n",
    "# sys.path.append(\"/home/afluo/size_figures\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import PIL.Image\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from utils import default_paths, roi_utils, prf_utils\n",
    "from model_fitting import initialize_fitting\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356939fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cortex\n",
    "from cortex import config\n",
    "import numpy as np\n",
    "# from plotter_utils import volume_maker, visualize_volume, view_1, view_2, make_image, close_handle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.getcwd()\n",
    "\n",
    "import nibabel as nib\n",
    "def load_from_nii(nii_file):\n",
    "    return nib.load(nii_file).get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbd4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_paths.beta_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsd_root = default_paths.nsd_root;\n",
    "stim_root = default_paths.stim_root\n",
    "beta_root = default_paths.beta_root\n",
    "\n",
    "trials_per_sess = 750\n",
    "sess_per_subj = 40\n",
    "# hard coded values based on sessions that are missing for some subs\n",
    "max_sess_each_subj = [40,40,32,30,40,32,40,30]\n",
    "\n",
    "def get_session_inds_full():\n",
    "    session_inds = np.repeat(np.arange(0,sess_per_subj), trials_per_sess)\n",
    "    return session_inds\n",
    "\n",
    "def load_from_nii(nii_file):\n",
    "    return nib.load(nii_file).get_fdata()\n",
    "  \n",
    "def load_from_mgz(mgz_file):\n",
    "    return load_from_nii(mgz_file)\n",
    "  \n",
    "def load_from_hdf5(hdf5_file, keyname=None):\n",
    "    data_set = h5py.File(hdf5_file, 'r')\n",
    "    if keyname is None:\n",
    "        keyname = list(data_set.keys())[0]\n",
    "    values = np.copy(data_set[keyname])\n",
    "    data_set.close()    \n",
    "    return values\n",
    "\n",
    "def image_uncolorize_fn(image):\n",
    "    data = image.astype(np.float32) / 255\n",
    "    if data.shape[2]==3:\n",
    "        bw = (0.2126*data[:,:,0:1]+ 0.7152*data[:,:,1:2]+ 0.0722*data[:,:,2:3])\n",
    "    elif data.shape[1]==3:\n",
    "        bw = (0.2126*data[:,0:1]+ 0.7152*data[:,1:2]+ 0.0722*data[:,2:3])\n",
    "    \n",
    "    return bw\n",
    "\n",
    "def image_preproc_fn(image):\n",
    "    data = image.astype(np.float32) / 255\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_voxel_mask(subject):\n",
    "    \n",
    "    voxel_mask, voxel_index, voxel_roi, voxel_ncsnr, brain_nii_shape = \\\n",
    "                roi_utils.get_voxel_roi_info(subject, which_hemis='concat')\n",
    "    \n",
    "    return voxel_mask\n",
    "\n",
    "\n",
    "def ncsnr_to_nc(ncsnr, average_image_reps=False, subject=None):    \n",
    "    \"\"\"\n",
    "    From Allen (2021) nature neuroscience.    \n",
    "    \"\"\"   \n",
    "    if not average_image_reps: \n",
    "        # single trial data\n",
    "        n = 1\n",
    "        noise_ceiling = 100 * ncsnr**2 / (ncsnr**2 + 1/n)  \n",
    "    else:\n",
    "        if subject is None:\n",
    "            # assume averaging over three reps of each image\n",
    "            n = 3\n",
    "            noise_ceiling = 100 * ncsnr**2 / (ncsnr**2 + 1/n)\n",
    "        else:\n",
    "            # for this subject, count how many reps of each image there actually were\n",
    "            # assume all available sessions are being used\n",
    "            image_order = get_master_image_order()    \n",
    "            session_inds = get_session_inds_full()\n",
    "            sessions = np.arange(max_sess_each_subj[subject-1])\n",
    "            inds2use = np.isin(session_inds, sessions)\n",
    "            image_order = image_order[inds2use]\n",
    "            unique, counts = np.unique(image_order, return_counts=True)\n",
    "            A = np.sum(counts==3)\n",
    "            B = np.sum(counts==2)\n",
    "            C = np.sum(counts==1)\n",
    "            # special version of the NC formula, from nsd data manual\n",
    "            noise_ceiling = 100 * ncsnr**2 / (ncsnr**2 + (A/3 + B/2 + C/1) / (A+B+C) )\n",
    "            \n",
    "    return noise_ceiling\n",
    "  \n",
    "def get_nc(subject, average_image_reps=True):\n",
    "    \n",
    "    # this is computed in roi_utils.preproc_rois()\n",
    "    if average_image_reps:\n",
    "        filename = os.path.join(default_paths.nsd_rois_root, 'S%d_noise_ceiling_avgreps.npy'%subject)\n",
    "    else:\n",
    "        filename = os.path.join(default_paths.nsd_rois_root, 'S%d_noise_ceiling_noavg.npy'%subject)\n",
    "        \n",
    "    noise_ceiling = np.load(filename)/100\n",
    "    \n",
    "    return noise_ceiling\n",
    "\n",
    "def get_image_data(subject, random_images=False, native=False, npix=240):\n",
    "\n",
    "    \"\"\"\n",
    "    Load the set of NSD images that were shown to a given subject.\n",
    "    This loads a subject-specific array of images, see [get_subject_specific_images] for details.\n",
    "    Can also choose to insert random noise instead of the images here.\n",
    "    \"\"\"\n",
    "    \n",
    "    if random_images==False:        \n",
    "        print('\\nLoading images for subject %d\\n'%subject)\n",
    "        if native:\n",
    "            image_data = load_from_hdf5(os.path.join(stim_root, 'S%d_stimuli_native.h5py'%subject))     \n",
    "        else:\n",
    "            image_data = load_from_hdf5(os.path.join(stim_root, 'S%d_stimuli_%d.h5py'%(subject,npix)))        \n",
    "    else:        \n",
    "        print('\\nGenerating random gaussian noise images...\\n')\n",
    "        n_images = 10000\n",
    "        image_data = (np.random.normal(0,1,[n_images, 3, npix,npix])*30+255/2).astype(np.uint8)\n",
    "        image_data = np.maximum(np.minimum(image_data, 255),0)\n",
    "\n",
    "    print ('image data size:', image_data.shape, ', dtype:', image_data.dtype, ', value range:',\\\n",
    "        np.min(image_data[0]), np.max(image_data[0]))\n",
    "\n",
    "    return image_data\n",
    "\n",
    "def get_master_image_order():    \n",
    "    \"\"\"\n",
    "    Gather the \"ordering\" information for NSD images.\n",
    "    masterordering gives zero-indexed ordering of indices (matlab-like to python-like), same for all subjects. \n",
    "    consists of 30000 values in the range [0-9999], which provide a list of trials in order. \n",
    "    The value in ordering[ii] tells the index into the subject-specific stimulus array that we would need to take to\n",
    "    get the image for that trial.\n",
    "    \"\"\"\n",
    "    exp_design_file = os.path.join(nsd_root, 'nsddata','experiments','nsd','nsd_expdesign.mat')\n",
    "    exp_design = loadmat(exp_design_file)\n",
    "    \n",
    "    image_order = exp_design['masterordering'].flatten() - 1 \n",
    "    \n",
    "    return image_order\n",
    "      \n",
    "\n",
    "def load_betas(subject, sessions=[0], voxel_mask=None,  zscore_betas_within_sess=True, volume_space=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Load preprocessed voxel data for an NSD subject (beta weights).\n",
    "    Always loading the betas with suffix 'fithrf_GLMdenoise_RR.\n",
    "    Concatenate the values across multiple sessions.\n",
    "    \"sessions\" is zero-indexed, add one to get the actual session numbers.\n",
    "    \"\"\"\n",
    "    \n",
    "    if volume_space:\n",
    "        beta_subj_folder = os.path.join(beta_root, 'subj%02d'%subject, 'func1pt8mm', 'betas_fithrf_GLMdenoise_RR')   \n",
    "    else:\n",
    "        beta_subj_folder = os.path.join(beta_root, 'subj%02d'%subject, 'nativesurface', 'betas_fithrf_GLMdenoise_RR')   \n",
    "\n",
    "    print('Data is located in: %s...'%beta_subj_folder)\n",
    "\n",
    "    if np.any((np.array(sessions)+1)>max_sess_each_subj[subject-1]):\n",
    "        print('attempting to load sessions:')\n",
    "        print(sessions+1)\n",
    "        raise ValueError('trying to load sessions that do not exist for subject %d, only has up to session %d'\\\n",
    "                         %(subject, max_sess_each_subj[subject-1]))\n",
    "        \n",
    "    n_trials = len(sessions)*trials_per_sess\n",
    "\n",
    "    for ss, se in enumerate(sessions):\n",
    "        try:\n",
    "            del betas\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            del values\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        gc.collect()\n",
    "        if volume_space:\n",
    "\n",
    "            # Load volume space nifti\n",
    "            fn2load = os.path.join(beta_subj_folder, 'betas_session%02d.nii.gz'%(se+1))\n",
    "            print('Loading from %s...'%fn2load)\n",
    "            values = load_from_nii(fn2load).transpose((3,0,1,2))\n",
    "            print('Raw data:')\n",
    "            print(values.dtype, np.min(values), np.max(values), values.shape)\n",
    "\n",
    "            betas = values.reshape((len(values), -1), order='C')\n",
    "\n",
    "        else:\n",
    "            # Surface space, concatenate the two hemispheres\n",
    "            # Must be left then right to match ROI definitions.\n",
    "            fn2load1 = os.path.join(beta_subj_folder, 'lh.betas_session%02d.hdf5'%(se+1))\n",
    "            fn2load2 = os.path.join(beta_subj_folder, 'rh.betas_session%02d.hdf5'%(se+1))\n",
    "\n",
    "            print('Loading from %s...'%fn2load1)        \n",
    "            values1 = load_from_hdf5(fn2load1)\n",
    "            print('Raw data:')\n",
    "            print(values1.dtype, np.min(values1), np.max(values1), values1.shape)\n",
    "\n",
    "            print('Loading from %s...'%fn2load2)        \n",
    "            values2 = load_from_hdf5(fn2load2)\n",
    "            print('Raw data:')\n",
    "            print(values2.dtype, np.min(values2), np.max(values2), values2.shape)\n",
    "\n",
    "            betas = np.concatenate((values1, values2), axis=1)\n",
    "\n",
    "        # divide by 300 to convert back to percent signal change\n",
    "        betas = betas.astype(np.float32) / 300\n",
    "\n",
    "        print('Adjusted data (divided by 300):')\n",
    "        print(betas.dtype, np.min(betas), np.max(betas), betas.shape)\n",
    "        \n",
    "        if voxel_mask is not None:        \n",
    "            betas = betas[:,voxel_mask]\n",
    "\n",
    "        if zscore_betas_within_sess: \n",
    "            print('z-scoring beta weights within this session...')\n",
    "            mb = np.mean(betas, axis=0, keepdims=True)\n",
    "            sb = np.std(betas, axis=0, keepdims=True)\n",
    "            betas = np.nan_to_num((betas - mb) / (sb + 1e-6))\n",
    "            print (\"mean = %.3f, sigma = %.3f\" % (np.mean(mb), np.mean(sb)))\n",
    "            del mb\n",
    "            del sb\n",
    "            gc.collect()\n",
    "\n",
    "        if ss==0:        \n",
    "            n_vox = betas.shape[1]\n",
    "            betas_full = np.zeros((n_trials, n_vox), dtype=np.single)\n",
    "\n",
    "        betas_full[ss*trials_per_sess : (ss+1)*trials_per_sess, :] = betas\n",
    "        \n",
    "    return betas_full\n",
    "       \n",
    "def get_concat_betas(subject, debug=False):\n",
    "    \n",
    "    print('\\nProcessing subject %d\\n'%subject)\n",
    "    voxel_mask = get_voxel_mask(subject)\n",
    "        \n",
    "    if debug:\n",
    "        sessions = [0]\n",
    "    else:\n",
    "        sessions = np.arange(max_sess_each_subj[subject-1])\n",
    "    zscore_betas_within_sess = True\n",
    "    volume_space=True\n",
    "    voxel_data = load_betas(subject, sessions, voxel_mask=voxel_mask, \\\n",
    "                                      zscore_betas_within_sess=zscore_betas_within_sess, \\\n",
    "                                      volume_space=volume_space)\n",
    "    print('\\nSize of full data set [nTrials x nVoxels] is:')\n",
    "    print(voxel_data.shape)\n",
    "\n",
    "    save_fn = os.path.join(default_paths.nsd_data_concat_root, 'S%d_allsess_concat_visual.h5py'%subject)\n",
    "\n",
    "    t = time.time()\n",
    "    print('saving file to %s'%save_fn)\n",
    "    with h5py.File(save_fn, 'w') as data_set:\n",
    "        dset = data_set.create_dataset(\"betas\", np.shape(voxel_data), dtype=np.float64)\n",
    "        data_set['/betas'][:,:] = voxel_data\n",
    "        data_set.close()  \n",
    "    elapsed = time.time() - t\n",
    "    print('took %.5f sec'%elapsed)\n",
    "    \n",
    "def average_image_repetitions(voxel_data, image_order):\n",
    "    \n",
    "    n_trials = voxel_data.shape[0]\n",
    "    n_voxels = voxel_data.shape[1]\n",
    "\n",
    "    unique_ims = np.unique(image_order)\n",
    "    print(unique_ims)\n",
    "    assert False\n",
    "    n_unique_ims = len(unique_ims)\n",
    "    avg_dat_each_image = np.zeros((n_unique_ims, n_voxels), dtype=np.single)\n",
    "    for uu, im in enumerate(unique_ims):\n",
    "        inds = image_order==im;\n",
    "        if np.sum(inds)<3:\n",
    "            print(im)\n",
    "        avg_dat_each_image[uu,:] = np.mean(voxel_data[inds,:], axis=0)\n",
    "        \n",
    "    return avg_dat_each_image, unique_ims\n",
    "    \n",
    "def get_data_splits(subject, sessions=[0], voxel_mask=None, \\\n",
    "                    zscore_betas_within_sess=True, volume_space=True, \\\n",
    "                    average_image_reps = False, \\\n",
    "                    shuffle_images=False, random_voxel_data=False):\n",
    "\n",
    "    \"\"\"\n",
    "    Gather voxel data and the indices of training/testing images, for one NSD subject.\n",
    "    Not actually loading images here, because all image features are pre-computed. \n",
    "    Always leaving out the \"shared1000\" image subset as my validation set, and training within the rest of the data.\n",
    "    Can specify a list of sessions to work with (don't have to be contiguous).\n",
    "    Can specify whether to work in volume or surface space (set volume_space to True or False).\n",
    "    Can also choose to shuffle images or generate random voxel data at this stage if desired.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the experiment design file that defines full image order over 30,000 trials\n",
    "    image_order = get_master_image_order()\n",
    "    \n",
    "    # Decide which sessions to work with here\n",
    "    session_inds = get_session_inds_full()\n",
    "    \n",
    "#     print(session_inds, session_inds.shape)\n",
    "#     print(np.unique(session_inds, return_counts=True))\n",
    "#     assert False\n",
    "    if np.isscalar(sessions):\n",
    "        sessions = [sessions]\n",
    "    sessions = np.array(sessions)    \n",
    "    if np.any((sessions+1)>max_sess_each_subj[subject-1]):\n",
    "        # adjust the session list that was entered, if the subject is missing some sessions.\n",
    "        # will alter the list for both images and voxel data.\n",
    "        print('subject %d only has up to session %d, will load these sessions:'%\\\n",
    "              (subject, max_sess_each_subj[subject-1]))\n",
    "        sessions = sessions[(sessions+1)<=max_sess_each_subj[subject-1]]\n",
    "        print(sessions+1)\n",
    "        assert(len(sessions)>0)\n",
    "        \n",
    "    inds2use = np.isin(session_inds, sessions)\n",
    "    session_inds = session_inds[inds2use]\n",
    "    image_order = image_order[inds2use]\n",
    "    \n",
    "    unique_ims = np.unique(image_order)\n",
    "    print(unique_ims)\n",
    "    n_unique_ims = len(unique_ims)\n",
    "    for uu, im in enumerate(unique_ims):\n",
    "        inds = image_order==im;\n",
    "        if np.sum(inds)<3:\n",
    "            print(np.sum(inds), uu)        \n",
    "\n",
    "    assert False\n",
    "    # Now load voxel data (preprocessed beta weights for each trial)\n",
    "    print('Loading data for sessions:')\n",
    "    print(sessions+1)\n",
    "    if not random_voxel_data:\n",
    "        voxel_data = load_betas(subject, sessions, voxel_mask=voxel_mask, \\\n",
    "                            zscore_betas_within_sess=zscore_betas_within_sess, \\\n",
    "                            volume_space=volume_space)\n",
    "    else:\n",
    "        print('creating random normally distributed data instead of loading real data')\n",
    "        if voxel_mask is not None:\n",
    "            n_voxels = np.sum(voxel_mask)\n",
    "        else:\n",
    "            n_voxels = 10000\n",
    "        voxel_data = np.random.normal(0,1,(len(image_order), n_voxels))\n",
    "    print(voxel_data.shape, \"Voxel data shape\")\n",
    "    print('\\nSize of full data set [n_trials x n_voxels] is:')\n",
    "    print(voxel_data.shape)\n",
    "    assert(voxel_data.shape[0]==len(image_order))\n",
    "    \n",
    "    # average over repetitions of same image, if desired\n",
    "    if average_image_reps:\n",
    "        avg_dat_each_image, unique_ims = average_image_repetitions(voxel_data, image_order)\n",
    "        voxel_data = avg_dat_each_image # use average data going forward\n",
    "        image_order = unique_ims # now the unique image indices become new image order\n",
    "        # NOTE that the unique images can be fewer than 10,000 if the subject\n",
    "        # is missing some data, or if we are working w just a few sessions. \n",
    "        print('\\nAfter averaging - size of full data set [n_images x n_voxels] is:')\n",
    "        print(voxel_data.shape)\n",
    "        # can't have session inds here because betas are averaged over multiple sessions\n",
    "        session_inds=None\n",
    "        \n",
    "   \n",
    "    # Get indices to split into training/validation set now\n",
    "    subj_df = get_subj_df(subject)\n",
    "    is_shared_image = np.array(subj_df['shared1000'])\n",
    "    shared_1000_inds = is_shared_image[image_order]\n",
    "    val_inds = shared_1000_inds\n",
    "    \n",
    "    is_trn, is_holdout, is_val = load_image_data_partitions(subject)\n",
    "    is_trn = is_trn[image_order]\n",
    "    is_val = is_val[image_order]\n",
    "    is_holdout = is_holdout[image_order]\n",
    "    assert(np.all(is_val==val_inds))\n",
    "    holdout_inds = is_holdout\n",
    "    \n",
    "    if shuffle_images:\n",
    "        print('\\nShuffling image order')\n",
    "        # shuffle each data partition separately\n",
    "        for inds in [is_trn, is_holdout, is_val]:\n",
    "            values = image_order[inds]\n",
    "            image_order[inds] = values[np.random.permutation(len(values))]      \n",
    "\n",
    "    return voxel_data, image_order, val_inds, holdout_inds, session_inds\n",
    "   \n",
    "\n",
    "def resize_image_tensor(x, newsize):\n",
    "        tt = x.transpose((0,2,3,1))\n",
    "        r  = np.ndarray(shape=x.shape[:1]+newsize+(x.shape[1],), dtype=tt.dtype) \n",
    "        for i,t in enumerate(tt):\n",
    "            r[i] = np.asarray(PIL.Image.fromarray(t).resize(newsize, resample=PIL.Image.BILINEAR))\n",
    "        return r.transpose((0,3,1,2))   \n",
    "\n",
    "def get_subject_specific_images(nsd_root, path_to_save, npix=227, debug=False):\n",
    "\n",
    "    \"\"\" \n",
    "    Load the big array of NSD images for all subjects.\n",
    "    Downsample to a desired size, and select just those viewed by a given subject.\n",
    "    Save a smaller array for each subject, at specified path.\n",
    "    \"\"\"\n",
    "    \n",
    "    stim_file_original = os.path.join(nsd_root,\"nsddata_stimuli/stimuli/nsd/nsd_stimuli.hdf5\")\n",
    "    exp_design_file = os.path.join(nsd_root,\"nsddata/experiments/nsd/nsd_expdesign.mat\")\n",
    "    exp_design = loadmat(exp_design_file)\n",
    "    subject_idx  = exp_design['subjectim']\n",
    "    if debug:\n",
    "        subject_idx = subject_idx[0:1]\n",
    "        \n",
    "    print (\"Loading full block of images...\")\n",
    "    image_data_set = h5py.File(stim_file_original, 'r')\n",
    "    print(image_data_set.keys())\n",
    "    image_data = np.copy(image_data_set['imgBrick'])\n",
    "    image_data_set.close()\n",
    "    print(image_data.shape)\n",
    "\n",
    "    for k,s_idx in enumerate(subject_idx):\n",
    "        fn2save = os.path.join(path_to_save, 'S%d_stimuli_%d'%(k+1, npix))\n",
    "        print('Will save to %s'%fn2save)       \n",
    "        print('Resizing...')\n",
    "        s_image_data = image_data[s_idx - 1]\n",
    "        s_image_data = resize_image_tensor(s_image_data.transpose(0,3,1,2), newsize=(npix,npix))\n",
    "        print(s_image_data.shape)        \n",
    "        print('saving to %s'%fn2save)\n",
    "        \n",
    "        with h5py.File(fn2save + '.h5py', 'w') as hf:\n",
    "            key='stimuli'\n",
    "            val=s_image_data        \n",
    "            hf.create_dataset(key,data=val)\n",
    "            print ('saved %s in h5py file' %(key))\n",
    "\n",
    "\n",
    "def get_subj_df(subject):\n",
    "    \"\"\"\n",
    "    Get info about the 10,000 images that were shown to each subject.\n",
    "    Note this is not the full ordered sequence of trials (which is 30,000 long)\n",
    "    This is only the unique images \n",
    "    (matches what is in /user_data/mmhender/nsd_stimuli/stimuli/nsd/S1_stimuli....h5py)\n",
    "    \"\"\"\n",
    "    exp_design_file = os.path.join(nsd_root,\"nsddata/experiments/nsd/nsd_expdesign.mat\")\n",
    "    exp_design = loadmat(exp_design_file)\n",
    "    subject_idx  = exp_design['subjectim']\n",
    "    \n",
    "    nsd_meta_file = os.path.join(nsd_root, 'nsddata/experiments/nsd/nsd_stim_info_merged.pkl')\n",
    "    with open(nsd_meta_file,'rb') as f:\n",
    "        stim_info = pickle.load(f,encoding=\"latin1\")\n",
    "    \n",
    "    ss=subject-1\n",
    "    subject_df = stim_info.loc[subject_idx[ss,:]-1]\n",
    "\n",
    "    return subject_df\n",
    "\n",
    "\n",
    "def load_prf_mapping_pars(subject, voxel_mask=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Load parameters of pRF fits for each voxel, obtained during independent pRF mapping expt.\n",
    "    Stimuli are sweeping bars w objects, see:\n",
    "    https://natural-scenes-dataset.s3-us-east-2.amazonaws.com/nsddata/experiments/prf/prf_screencapture.mp4\n",
    "    \"\"\"\n",
    "    \n",
    "    if voxel_mask is None:\n",
    "        voxel_mask = get_voxel_mask(subject)\n",
    "        \n",
    "    prf_path = os.path.join(default_paths.nsd_root, 'nsddata','ppdata','subj%02d'%subject,'func1pt8mm')\n",
    "\n",
    "    angle = load_from_nii(os.path.join(prf_path, 'prf_angle.nii.gz')).flatten()[voxel_mask]\n",
    "    eccen = load_from_nii(os.path.join(prf_path, 'prf_eccentricity.nii.gz')).flatten()[voxel_mask]\n",
    "    size = load_from_nii(os.path.join(prf_path, 'prf_size.nii.gz')).flatten()[voxel_mask]\n",
    "    exponent = load_from_nii(os.path.join(prf_path, 'prf_exponent.nii.gz')).flatten()[voxel_mask]\n",
    "    gain = load_from_nii(os.path.join(prf_path, 'prf_gain.nii.gz')).flatten()[voxel_mask]\n",
    "    rsq = load_from_nii(os.path.join(prf_path, 'prf_R2.nii.gz')).flatten()[voxel_mask]/100\n",
    "            \n",
    "    return angle, eccen, size, exponent, gain, rsq\n",
    "\n",
    "\n",
    "def load_domain_tvals(subject, voxel_mask=None):\n",
    "\n",
    "    \"\"\"\n",
    "    For one NSD subject, load the t-statistics for all domain contrasts \n",
    "    from independent localizer task (faces, places, etc)\n",
    "    \"\"\"\n",
    "    \n",
    "    if voxel_mask is None:\n",
    "        voxel_mask = get_voxel_mask(subject)\n",
    "        \n",
    "    n_voxels = np.sum(voxel_mask)\n",
    "\n",
    "    niftis_path = os.path.join(default_paths.nsd_root, \\\n",
    "                               'nsddata', 'ppdata','subj%02d'%subject, 'func1pt8mm')\n",
    "\n",
    "    categ_list = ['places', 'faces', 'bodies', 'objects', 'characters']\n",
    "    n_categ = len(categ_list)\n",
    "\n",
    "    tvals_all = np.zeros((n_voxels, n_categ))\n",
    "\n",
    "    for cc, categ in enumerate(categ_list):\n",
    "\n",
    "        # load t-statistics for the domain contrast of interest\n",
    "        tvals_filename = os.path.join(niftis_path, 'floc_%stval.nii.gz'%categ)\n",
    "        tvals = load_from_nii(tvals_filename)\n",
    "        tvals = tvals.reshape((1, -1), order='C')[0]\n",
    "\n",
    "        # pull out same set of voxels all my analyses were done on\n",
    "        tvals_masked = tvals[voxel_mask] \n",
    "\n",
    "        tvals_all[:,cc] = tvals_masked\n",
    "\n",
    "    return tvals_all, categ_list\n",
    "\n",
    "\n",
    "def get_image_ranks(subject, sessions=np.arange(0,40), debug=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    For each voxel, rank images in order of average response \n",
    "    (averaged over duplicate trials) and save as a csv file.\n",
    "    Each column in csv is a voxel, each row is a rank position.\n",
    "    \"\"\"\n",
    "\n",
    "    if np.isscalar(sessions):\n",
    "        sessions = [sessions]\n",
    "    sessions = np.array(sessions)\n",
    "    if np.any((sessions+1)>max_sess_each_subj[subject-1]):\n",
    "        # adjust the session list that was entered, if the subject is missing some sessions.\n",
    "        # will alter the list for both images and voxel data.\n",
    "        print('subject %d only has up to session %d, will load these sessions:'%(subject, max_sess_each_subj[subject-1]))\n",
    "        sessions = sessions[(sessions+1)<=max_sess_each_subj[subject-1]]\n",
    "        print(sessions+1)\n",
    "        assert(len(sessions)>0)\n",
    "    if debug:\n",
    "        sessions = np.array([0])\n",
    "        \n",
    "    voxel_mask = get_voxel_mask(subject)\n",
    "        \n",
    "    voxel_data = load_betas(subject, sessions, voxel_mask=voxel_mask, \\\n",
    "                              zscore_betas_within_sess=True, \\\n",
    "                              volume_space=True)    \n",
    "    image_order = get_master_image_order()\n",
    "    session_inds = get_session_inds_full()\n",
    "\n",
    "    inds2use = np.isin(session_inds, sessions)\n",
    "    image_order = image_order[inds2use]\n",
    "\n",
    "    n_trials = voxel_data.shape[0]\n",
    "    n_voxels = voxel_data.shape[1]\n",
    "\n",
    "    unique_ims = np.unique(image_order)\n",
    "    n_unique_ims = len(unique_ims)\n",
    "    avg_dat_each_image = np.zeros((n_unique_ims, n_voxels))\n",
    "    for uu, im in enumerate(unique_ims):\n",
    "        if debug and (uu>1):\n",
    "            continue\n",
    "        inds = image_order==im;\n",
    "        avg_dat_each_image[uu,:] = np.mean(voxel_data[inds,:], axis=0)\n",
    "\n",
    "\n",
    "    images_ranked_each_voxel = np.zeros((n_unique_ims, n_voxels))\n",
    "    for vv in range(n_voxels):\n",
    "        if debug and (vv>1):\n",
    "            continue\n",
    "        image_rank = np.flip(np.argsort(avg_dat_each_image[:,vv]))\n",
    "        images_ranked = unique_ims[image_rank]\n",
    "        images_ranked_each_voxel[:,vv] = images_ranked\n",
    "\n",
    "    rank_df = pd.DataFrame(data=images_ranked_each_voxel.astype(int), \\\n",
    "                           columns=['voxel %d'%vv for vv in range(n_voxels)])\n",
    "\n",
    "    fn2save = os.path.join(default_paths.stim_root, 'S%d_ranked_images.csv'%subject)\n",
    "    print('Saving to %s'%fn2save)\n",
    "    rank_df.to_csv(fn2save, header=True)\n",
    "    \n",
    "def load_image_data_partitions(subject):\n",
    "    \n",
    "    fn2load = os.path.join(default_paths.stim_root, 'Image_data_partitions.npy')\n",
    "    print('loading train/holdout/val image list from %s'%fn2load)\n",
    "    partitions = np.load(fn2load, allow_pickle=True).item()\n",
    "    is_trn = partitions['is_trn'][:,subject-1]\n",
    "    is_holdout = partitions['is_holdout'][:,subject-1]\n",
    "    is_val = partitions['is_val'][:,subject-1]\n",
    "    \n",
    "    return is_trn, is_holdout, is_val\n",
    "\n",
    "def make_image_data_partitions(pct_holdout=0.10):\n",
    "\n",
    "    subjects=np.concatenate([np.arange(1,9),[999]], axis=0)\n",
    "    n_subjects = len(subjects)\n",
    "    # fixed random seeds for each subject, to make sure shuffling is repeatable\n",
    "    rndseeds = [171301, 42102, 490304, 521005, 11407, 501610, 552211, 450013, 824387]\n",
    "    \n",
    "    n_images_total = 10000\n",
    "    is_trn = np.zeros((n_images_total,n_subjects),dtype=bool)\n",
    "    is_holdout = np.zeros((n_images_total,n_subjects),dtype=bool)\n",
    "    is_val = np.zeros((n_images_total,n_subjects),dtype=bool)\n",
    "\n",
    "    for si, ss in enumerate(subjects):\n",
    "\n",
    "        if ss==999:\n",
    "            val_image_inds = np.arange(0,10000)<1000\n",
    "            trn_image_inds = np.arange(0,10000)>=1000            \n",
    "        else:           \n",
    "            subject_df = get_subj_df(ss)\n",
    "            val_image_inds = subject_df['shared1000']\n",
    "            trn_image_inds = ~subject_df['shared1000']\n",
    "\n",
    "        n_images_val = np.sum(val_image_inds)\n",
    "        n_images_notval = np.sum(trn_image_inds);\n",
    "        n_images_holdout = int(np.ceil(n_images_notval*pct_holdout))\n",
    "        n_images_trn = n_images_notval - n_images_holdout\n",
    "\n",
    "        # of the full 9000 image training set, holding out a random chunk\n",
    "        inds_notval = np.where(trn_image_inds)[0]\n",
    "        np.random.seed(rndseeds[si])\n",
    "        np.random.shuffle(inds_notval)\n",
    "        inds_trn = inds_notval[0:n_images_trn]\n",
    "        inds_holdout = inds_notval[n_images_trn:]\n",
    "        assert(len(inds_holdout)==n_images_holdout)\n",
    "\n",
    "        is_trn[inds_trn,si] = 1\n",
    "        is_holdout[inds_holdout,si] = 1\n",
    "        is_val[val_image_inds,si] = 1\n",
    "\n",
    "    fn2save = os.path.join(default_paths.stim_root, 'Image_data_partitions.npy')\n",
    "    np.save(fn2save, {'is_trn': is_trn, \\\n",
    "                      'is_holdout': is_holdout, \\\n",
    "                      'is_val': is_val, \\\n",
    "                      'rndseeds': rndseeds})\n",
    "    \n",
    "    \n",
    "def discretize_mappingtask_prfs(which_prf_grid=5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Converting pRF definitions from the pRF mapping task (which are continous)\n",
    "    into the closest parameters from a grid of pRFs\n",
    "    Can be used for fitting models\n",
    "    \"\"\"\n",
    "\n",
    "    prf_grid = initialize_fitting.get_prf_models(which_prf_grid).round(3)\n",
    "    grid_x_deg, grid_y_deg = prf_grid[:,0]*8.4, prf_grid[:,1]*8.4\n",
    "    grid_size_deg = prf_grid[:,2]*8.4\n",
    "\n",
    "    subjects = np.arange(1,9)\n",
    "    for si,ss in enumerate(subjects):\n",
    "\n",
    "        voxel_mask = get_voxel_mask(subject=ss)\n",
    "        n_vox = np.sum(voxel_mask)\n",
    "\n",
    "        a,e,s, exp,gain,rsq = load_prf_mapping_pars(subject=ss, voxel_mask = voxel_mask)\n",
    "        x_mapping, y_mapping = prf_utils.pol_to_cart(a,e)\n",
    "        x_mapping = np.minimum(np.maximum(x_mapping, -7), 7)\n",
    "        y_mapping = np.minimum(np.maximum(y_mapping, -7), 7)\n",
    "        s_mapping = np.minimum(s, 8.4)\n",
    "        \n",
    "        print('there are %d nans in x_mapping'%np.sum(np.isnan(x_mapping)))\n",
    "        print('there are %d nans in y_mapping'%np.sum(np.isnan(y_mapping)))\n",
    "        print('there are %d nans in s_mapping'%np.sum(np.isnan(s_mapping)))\n",
    "        x_mapping[np.isnan(x_mapping)] = 0\n",
    "        y_mapping[np.isnan(y_mapping)] = 0\n",
    "\n",
    "        prf_grid_inds = np.zeros((n_vox,1),dtype=int)\n",
    "\n",
    "        for vv in range(n_vox):\n",
    "\n",
    "            # first find the [x,y] coordinate closest to this pRF center (in my grid)\n",
    "            distances_xy = np.sqrt((x_mapping[vv]-grid_x_deg)**2 + (y_mapping[vv]-grid_y_deg)**2)\n",
    "\n",
    "            # should be multiple possible values here, for the different sizes\n",
    "            closest_xy_inds = np.where(distances_xy==np.min(distances_xy))[0]\n",
    "\n",
    "            # then find which size is closest to mapping task estimate\n",
    "            distances_size = np.abs(s_mapping[vv] - grid_size_deg[closest_xy_inds])\n",
    "\n",
    "            closest_ind = closest_xy_inds[np.argmin(distances_size)]\n",
    "\n",
    "            prf_grid_inds[vv] = closest_ind.astype(int)\n",
    "\n",
    "        save_prfs_folder = os.path.join(default_paths.save_fits_path, 'S%02d'%ss, 'mapping_task_prfs_grid%d'%which_prf_grid)\n",
    "\n",
    "        if not os.path.exists(save_prfs_folder):\n",
    "            os.makedirs(save_prfs_folder)\n",
    "\n",
    "        save_filename = os.path.join(save_prfs_folder, 'prfs.npy')\n",
    "        print('saving to %s'%save_filename)\n",
    "        np.save(save_filename, {'voxel_mask': voxel_mask,  \\\n",
    "                                'prf_grid_inds': prf_grid_inds, \\\n",
    "                                'prf_grid_pars': prf_grid})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cabc27-15a8-4b56-8e94-eb7ebf18f683",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the block of below to compute NSD voxel-wise values for each image\n",
    "The output is a numpy array of 1D vectors (flattened output of the 3D voxel grid brain). The output is quite large, so I recommend having a generous amount of storage (100GB) and RAM (at least 64GB) to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92d576",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for subj in [1,2,3,4,5,6,7,8]:\n",
    "    print()\n",
    "    print(\"Starting subject \", subj)\n",
    "    outs = get_data_splits(subj,sessions=list(range(40)), average_image_reps=True)\n",
    "    # assert False\n",
    "    np.save(\"/lab_data/tarrlab/afluo/NSD_zscored/subj_{}\".format(subj), outs[0])\n",
    "    np.save(\"/lab_data/tarrlab/afluo/NSD_zscored/subj_{}_order\".format(subj), outs[1])\n",
    "    del outs\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e113c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_idx = 1\n",
    "\n",
    "# nsd_root = '/lab_data/tarrlab/common/datasets/NSD'\n",
    "# nsd_meta_file = os.path.join(nsd_root, 'nsddata/experiments/nsd/nsd_stim_info_merged.pkl')\n",
    "# with open(nsd_meta_file,'rb') as f:\n",
    "#     stim_info = pickle.load(f,encoding=\"latin1\")\n",
    "# exp_design_file = os.path.join(nsd_root,\"nsddata/experiments/nsd/nsd_expdesign.mat\")\n",
    "# exp_design = loadmat(exp_design_file)\n",
    "# subject_idx_MATRIX = exp_design['subjectim']\n",
    "# subject_df = stim_info.loc[subject_idx_MATRIX[subject_idx-1,:]-1]\n",
    "# COCO_ids = np.array(subject_df[\"cocoId\"].tolist()).astype(np.int64).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a17031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e0155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_obj = pd.read_csv(\"/lab_data/tarrlab/common/datasets/NSD/nsddata/experiments/nsd/nsd_stim_info_merged.csv\")\n",
    "def coco_crop(img,cropbox_in):\n",
    "    if type(cropbox_in) is str:\n",
    "        cropbox = eval(cropbox_in)\n",
    "    else:\n",
    "        cropbox = cropbox_in\n",
    "    top = int(img.shape[0]*cropbox[0])\n",
    "    bottom = int(img.shape[0]*(1-cropbox[1]))\n",
    "    left = int(img.shape[1]*cropbox[2])\n",
    "    right = int(img.shape[1]*(1-cropbox[3]))\n",
    "    #     print(left, right,top, bottom,right-left, bottom-top, img.shape, img[left:right,top:bottom].shape)\n",
    "    return img[top:bottom, left:right]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c20317",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_path = \"/lab_data/tarrlab/common/datasets/COCO\"\n",
    "\n",
    "def get_cocopath(coco_id_):\n",
    "    if not isinstance(coco_id_, int):\n",
    "        mycoco_id = str(int(coco_id_))\n",
    "    else:\n",
    "        mycoco_id = str(coco_id_)\n",
    "    k = mycoco_id.zfill(12)+\".jpg\"\n",
    "    f1 = os.path.join(coco_path, \"train2017\", k) \n",
    "    f2 = os.path.join(coco_path, \"test2017\", k) \n",
    "    f3 = os.path.join(coco_path, \"val2017\", k) \n",
    "    if os.path.isfile(f1):\n",
    "        ff = f1\n",
    "\n",
    "    if os.path.isfile(f2):\n",
    "        ff = f2\n",
    "\n",
    "    if os.path.isfile(f3):\n",
    "        ff = f3\n",
    "    return ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3281da70",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coco = list(csv_obj[\"cocoId\"])\n",
    "import imageio\n",
    "import os\n",
    "from skimage.transform import rescale, resize\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f56ba-f9ac-439a-8ff2-d968f901a2b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run the section below to get the images used for NSD experiments\n",
    "Output will be uint8, between 0 and 255, resized to 256x256 for space reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f24e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File(\"/scratch/image_data.h5py\", 'w')\n",
    "zz = 0\n",
    "for i in all_coco:\n",
    "    zz += 1\n",
    "    if zz %100 == 0:\n",
    "        print(zz)\n",
    "    current_box = list(csv_obj[csv_obj[\"cocoId\"]==int(i)][\"cropBox\"])[0]\n",
    "    example_img = imageio.v2.imread(get_cocopath(i))\n",
    "    cropped_image = np.array(coco_crop(example_img, current_box))\n",
    "#     print(cropped_image.dtype)\n",
    "    cropped_image = Image.fromarray(cropped_image)\n",
    "    output = np.array(cropped_image.resize((256, 256), resample=Image.Resampling.BICUBIC))\n",
    "    # print(output.dtype, np.max(output), np.min(output))\n",
    "    file.create_dataset(str(i).zfill(12), data=output[:])\n",
    "#     plt.imshow(example_img)\n",
    "#     plt.show()\n",
    "#     plt.imshow(cropped_image)\n",
    "#     plt.show()\n",
    "#     plt.imshow(output)\n",
    "#     plt.show()\n",
    "#     print(cropped_image.shape)\n",
    "#     break\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27987229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c2b0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = h5py.File(\"/scratch/image_data.h5py\", 'r')\n",
    "# keys = list(file.keys())\n",
    "# zz = 0\n",
    "# for k in keys:\n",
    "#     plt.imshow(file[k][:])\n",
    "#     print(file[k][:].dtype)\n",
    "#     plt.show()\n",
    "#     zz += 1\n",
    "#     if zz == 10:\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
